{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Concepts\n",
    "\n",
    "## Mapper - Read file\n",
    "For each Text we create a set of k-shingles\n",
    "we need to pic the k carefulls so any given shingle is unique and we don't have too many shingles k=5 seems to be a good choice\n",
    "\n",
    "## Mapper - make shingle-sets\n",
    "We hash the shingles to reduce data size and to make the comparison of the shingles easier\n",
    "\n",
    "\n",
    "---\n",
    "##### What if all shingle sets don't fit into main memory and we cannot make a universal-set?\n",
    "---\n",
    "\n",
    "*In map reduce we don't need to make a universal-set and calculate Bit-Vectors because we use Min-Hashing to approximate the Jaccard-Distance of two Sets ,this works because if a Hash function can put each shingle into a bucket and we have as many or more buckets then Shingles in the master set, then the hash function simulates the master set and because we use the min() property this is the same as permuting the characteristic Matrix*\n",
    "\n",
    "Hierzu hilft es sich vorzustellen, dass jeder bucket der minhash funktion einer Zeile der Charakteristischen Matrix entspricht, und durch die min() eigenschaft wir dann quasi eine Permutation auf allen Spalten gleichzeitig machen, da gleiche Elemente den gleichen Hashwert haben und somit in den gleichen Bucket kommen, wir ignorieren hier, dass wir auch hash-kollisionen haben können, da wir eine sehr große anzahl an buckets haben und die wahrscheinlichkeit für eine kollision sehr gering ist, oder im Zweifel zu einem Falsch positiven führt, was nicht schlimm ist, da wir ja nur eine Approximation der Jaccard-Distanz wollen.\n",
    "\n",
    "## Mapper - calculate the MinHash-Signature for shingle set\n",
    "We don't need to calculate the Characteristic Matrix because we use Min-Hashing to approximate the Jaccard-Distance of two Sets we just need to find the right bucket-size therfore approximate the maximum number of shingles(or use a very large prime number ?)\n",
    "\n",
    "\n",
    "## Mapper - calculate the LSH-Bucket-number (key= bucket_number, value=doc_id)\n",
    "We use b Bands with size #hash_functions / b and hash each band to a bucket -> we get the following key-value pairs for the reducer (key= bucket_number, value=doc_id)\n",
    "\n",
    "## Reducer - find candidate pairs\n",
    "Since the reducer gets a list of doc_id´s that got hased into the same bucket each reducer can directly emit (yield) all possible pairs of doc_id´s that got hashed into the same bucket and we have the desired output (doc_id1, doc_id2) of a potential pair of similar documents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6ea51ae415fa1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data\n",
    " * Zuerst laden wir die Daten (Die beiden Texte aus der Eingabedatei)\n",
    " * Jeder Text wird einzeln betrachtet und in einer Liste gespeichert"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "600e21655e7fdf21"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T20:50:56.691647600Z",
     "start_time": "2023-11-10T20:50:54.439946200Z"
    }
   },
   "id": "d6e76a28f0ab6ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.690648700Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a list of all the texts\n",
    "texts = []\n",
    "with open('sts2016-english-with-gs-v1.0/STS2016.input.answer-answer.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        elements = line.strip().split('\\t')\n",
    "        texts.append(elements[0].replace('\\n', ''))\n",
    "        texts.append(elements[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    print(texts[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T20:50:56.693640700Z",
     "start_time": "2023-11-10T20:50:56.691647600Z"
    }
   },
   "id": "4efb76de42092b15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Erstellen der Shingles für jeden Text\n",
    " * Im ersten Map Schritt erstellen wir für jeden Text eine Liste von Shingles\n",
    " * Als key können wir die Texte verwenden, da wir diese später wieder brauchen\n",
    " * Der Value ist eine Liste von Shingles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42174621118a37bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shingles_per_text = []\n",
    "k = 3\n",
    "for text in texts:\n",
    "    shingles_per_text.append(set([text[i:i + k] for i in range(len(text) - k + 1)]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.693640700Z"
    }
   },
   "id": "29bbd8a3898b7ca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Bit-Vector for each text (Matrix of Bit-Vectors [f. 51])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1d3d69cda4c4ea9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a set of all shingles\n",
    "all_shingles_of_file = set()\n",
    "for shingle_of_text in shingles_per_text:\n",
    "    all_shingles_of_file.update(shingle_of_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.695635500Z"
    }
   },
   "id": "3f2f0cf4dbaa57e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create Bit-Vector for each text -> this will yield the matrix of the bit-vectors\n",
    "bit_Matrix = []\n",
    "for shingle_of_text in shingles_per_text:\n",
    "    bit_vector_for_text = []\n",
    "    \n",
    "    for shingle_of_file in all_shingles_of_file:\n",
    "        \n",
    "        if shingle_of_file in shingle_of_text:\n",
    "            bit_vector_for_text.append(1)\n",
    "        else:\n",
    "            bit_vector_for_text.append(0)\n",
    "            \n",
    "    bit_Matrix.append(bit_vector_for_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.698628100Z"
    }
   },
   "id": "58ffbfd601fb83cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nur um zz zeigen, dass es funktioniert\n",
    "for i in range(0, 5):\n",
    "    print(bit_Matrix[i][0:8])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.700622500Z"
    }
   },
   "id": "84585b723508587"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate MinHash-Signature for each text\n",
    "\n",
    "# Create Hash-Functions (random permutations)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a3c9497859b3c92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate n hash functions (random permutations)\n",
    "n = 500\n",
    "hash_functions = []\n",
    "for h in range(n):\n",
    "    perm = np.random.permutation(len(all_shingles_of_file))\n",
    "    hash_functions.append(perm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.702617500Z"
    }
   },
   "id": "18f06956bcf565f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate MinHash-Signature for each text (MinHash Matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34166d5f67254fb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "signature_matrix = []\n",
    "for bit_vector in bit_Matrix:\n",
    "    signature_vector = []\n",
    "    for hash_function in hash_functions:\n",
    "        # find the first 1 in the bit vector\n",
    "        # we don't permutate the original bit vector, we instead use the hash function and check the bit vector at the permutated index\n",
    "        for col_index in range(len(bit_vector)):\n",
    "            if bit_vector[hash_function[col_index]] == 1:\n",
    "                signature_vector.append(hash_function[col_index])\n",
    "                break\n",
    "    signature_matrix.append(signature_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.704612500Z"
    }
   },
   "id": "a0b2ec90c8f0699e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    print(signature_matrix[i][0:8])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.706607200Z"
    }
   },
   "id": "6ff016b4cc0e1ced"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare the MinHash-Signatures similarity to the Jaccard-Similarity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "819dc3d7a54573e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_similarities(index=0):\n",
    "    sig_1 = signature_matrix[index]\n",
    "    bit_v_1 = bit_Matrix[index]\n",
    "    sig_sims = []\n",
    "    bit_sims = []\n",
    "    \n",
    "    for col in signature_matrix:\n",
    "        sig_sims.append(np.sum(np.array(sig_1) == np.array(col)) / len(signature_matrix[0]))\n",
    "    \n",
    "    for col in bit_Matrix:\n",
    "        intersection = np.sum(np.array(bit_v_1) & np.array(col))\n",
    "        union = np.sum(np.array(bit_v_1) | np.array(col))\n",
    "        bit_sims.append(intersection / union)\n",
    "        \n",
    "    \n",
    "    return pd.DataFrame({'Jaccard-Similarity': bit_sims, 'MinHash-Similarity': sig_sims, 'Text': texts, 'difference': np.array(sig_sims) - np.array(bit_sims)})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.708601500Z"
    }
   },
   "id": "a11cb0cbe914553f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarities = find_similarities(index=2)\n",
    "similarities"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T20:50:56.769437700Z",
     "start_time": "2023-11-10T20:50:56.710596600Z"
    }
   },
   "id": "a701b64c099a197d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# find possible pairs of similar texts with LSH "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5a5ec041305abb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Bands and set Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b83102284157bfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# sort bands into buckets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c62772f951179375"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lsh_hash_function(band, num_buckets):\n",
    "    # Convert the band (which could be a list or tuple) to a string representation\n",
    "    # since the built-in hash function requires a single immutable argument.\n",
    "    string_band = str(band)\n",
    "\n",
    "    # Use the built-in hash function and take the modulo of the number of buckets\n",
    "    return hash(string_band) % num_buckets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.712591600Z"
    }
   },
   "id": "51570adfc4bcd660"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sort_bands_into_buckets(number_of_buckets=10000000, number_bands=64, number_rows_per_band=len(hash_functions) / 64):\n",
    "    buckets = dict()\n",
    "    for col_index in range(len(signature_matrix)):\n",
    "        for band_num in range(number_bands):\n",
    "            # get the signature vector for the current text\n",
    "            signature_vector = signature_matrix[col_index]\n",
    "            # get the current band\n",
    "            start_index = int(band_num * number_rows_per_band)\n",
    "            end_index = int((band_num + 1) * number_rows_per_band)\n",
    "            band = signature_vector[start_index:end_index]\n",
    "            # calculate the hash value for the band\n",
    "            bucket_index = lsh_hash_function(band, number_of_buckets)\n",
    "            # add the text to the bucket\n",
    "            buckets[bucket_index] = buckets.get(bucket_index, []) + [col_index]\n",
    "            \n",
    "    return buckets\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.715583700Z"
    }
   },
   "id": "b2e2025f2fb01e32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# find candidate pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f1cb86350d8fb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_candidate_pairs(buckets):    \n",
    "    candidate_pairs = set()\n",
    "    for bucket in buckets.values():\n",
    "        for i in range(len(bucket)):\n",
    "            for j in range(i + 1, len(bucket)):\n",
    "                candidate_pairs.add((bucket[i], bucket[j]))\n",
    "    return candidate_pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.717577700Z"
    }
   },
   "id": "6db0840eee76ddd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# compare number of candidate pairs for hyperparameters n_bands and n_buckets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d71de27ba014d45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_bands = [2, 4, 8, 16, 32, 64]\n",
    "number_rows_per_band = [len(hash_functions) / number_bands for number_bands in number_bands]\n",
    "number_of_buckets = [10000000, 100000000]\n",
    "\n",
    "# plot number of candidate pairs for number of bands going from 2 to 32 and number of buckets going from 100 to 10000000\n",
    "for n_bands in number_bands:\n",
    "    for n_buckets in number_of_buckets:\n",
    "        buckets = sort_bands_into_buckets(n_buckets, n_bands, len(hash_functions) / n_bands)\n",
    "        candidate_pairs = find_candidate_pairs(buckets)\n",
    "        print(f'Number of candidate pairs for {n_bands} bands and {n_buckets} buckets: {len(candidate_pairs)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.719572100Z"
    }
   },
   "id": "233c8c5c10acdc28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# get jaccard similarity for candidate pairs (this is extremely slow)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d492b1e0a09186"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jac_sims = []\n",
    "candidate_pairs = find_candidate_pairs(sort_bands_into_buckets())\n",
    "for pair in candidate_pairs:\n",
    "    jac_sims.append(np.sum(np.array(bit_Matrix[pair[0]]) & np.array(bit_Matrix[pair[1]])) / np.sum(np.array(bit_Matrix[pair[0]]) | np.array(bit_Matrix[pair[1]])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.723561600Z"
    }
   },
   "id": "983255b87578d81e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### create dataframe with jaccard similarities and text pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "928634294603cb61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "candidate_pairs_df = pd.DataFrame({'Jaccard-Similarity': jac_sims, 'id1': [pair[0] for pair in candidate_pairs], 'Text 1': [texts[pair[0]] for pair in candidate_pairs], 'id2': [pair[1] for pair in candidate_pairs], 'Text 2': [texts[pair[1]] for pair in candidate_pairs]})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.725556400Z"
    }
   },
   "id": "79cb1368bed701db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filter all cells, where id1 == id2\n",
    "candidate_pairs_df = candidate_pairs_df[candidate_pairs_df['id1'] != candidate_pairs_df['id2']]\n",
    "print(f'Number of possible pairs: {len(texts) * (len(texts) - 1)}')\n",
    "print(f'Number of found equals (sim=1): {len(candidate_pairs_df[candidate_pairs_df[\"Jaccard-Similarity\"] == 1])}')\n",
    "\n",
    "# remove jac_sims == 1\n",
    "candidate_pairs_df = candidate_pairs_df[candidate_pairs_df['Jaccard-Similarity'] != 1]\n",
    "\n",
    "print(f'Number of not equal candidate pairs: {len(candidate_pairs_df)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.729545200Z"
    }
   },
   "id": "7a2385ec777d3ecf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "candidate_pairs_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.733535500Z"
    }
   },
   "id": "7729341e93596906"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(candidate_pairs_df['Jaccard-Similarity'])\n",
    "plt.title('Histogram of Jaccard Similarities, buckets=10.000.000, bands=64')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.735529500Z"
    }
   },
   "id": "7f8f17ea292ddb24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.736526700Z"
    }
   },
   "id": "a0c2bcadb9ed1eeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-10T20:50:56.739518700Z"
    }
   },
   "id": "3dc773a014cc17a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
